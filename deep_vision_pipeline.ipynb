{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load libraries ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19f994e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dataloaders ===\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_csv, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.labels_df = pd.read_csv(label_csv)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, f'{self.labels_df.iloc[idx, 0]}.png')\n",
    "        image = Image.open(img_name)\n",
    "        label = self.labels_df.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize RGB\n",
    "])\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "image_folder = 'data/train'\n",
    "label_csv = 'data/train/labels_train.csv'\n",
    "dataset = ImageDataset(image_folder=image_folder, label_csv=label_csv, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Mimicking seminar code\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)     # No shuffling for validation\n",
    "\n",
    "# Initialize device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "406ce28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNN architecture ===\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=26):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        # Convolutional Layer Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Convolutional Layer Block 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Convolutional Layer Block 3 (Deeper layer)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # Batch normalization\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Convolutional Layer Block 4 (Even deeper layer)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)  # Batch normalization\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8, 512)  # Adjusted for the new size after pooling\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution block\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        # Second convolution block\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        # Third convolution block (deeper)\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        # Fourth convolution block (even deeper)\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        # Flatten the tensor before passing into fully connected layers\n",
    "        x = x.view(-1, 256 * 8 * 8)  # Flattening the output to pass into fc layers\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8452eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 79/79 [03:00<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 5.9302 Val Loss: 3.2601 Val Accuracy: 3.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 79/79 [02:53<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Train Loss: 3.2617 Val Loss: 3.2605 Val Accuracy: 3.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 79/79 [02:46<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Train Loss: 3.2577 Val Loss: 3.2611 Val Accuracy: 3.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 79/79 [02:42<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Train Loss: 3.2575 Val Loss: 3.2617 Val Accuracy: 3.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 79/79 [03:42<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Train Loss: 3.2571 Val Loss: 3.2623 Val Accuracy: 3.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 79/79 [03:05<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Train Loss: 3.2566 Val Loss: 3.2630 Val Accuracy: 3.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 79/79 [02:56<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Train Loss: 3.2570 Val Loss: 3.2636 Val Accuracy: 3.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 79/79 [02:52<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Train Loss: 3.2567 Val Loss: 3.2642 Val Accuracy: 3.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 79/79 [02:45<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Train Loss: 3.2564 Val Loss: 3.2645 Val Accuracy: 3.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:  77%|███████▋  | 61/79 [02:37<00:46,  2.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_losses, val_losses, val_accuracies\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Start training with subsets for 5 epochs to test\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m train_losses, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     84\u001b[0m     model, train_loader, val_loader, criterion, optimizer, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     85\u001b[0m )\n",
      "Cell \u001b[0;32mIn[42], line 44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 44\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     47\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Select a subset of the dataset (e.g., the first 1000 samples for training)\n",
    "subset_train_size = 2500  # Set to the number of samples you want to test\n",
    "subset_val_size = 800  # Set a smaller subset for validation\n",
    "\n",
    "# Get indices for the subsets (you can change the subset size as per your requirement)\n",
    "train_indices = list(range(subset_train_size))\n",
    "val_indices = list(range(subset_val_size))\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "# Create DataLoader for the subsets\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} \"\n",
    "              f\"Val Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "# Start training with subsets for 5 epochs to test\n",
    "train_losses, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa30cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 3.1673, Accuracy = 9.38%\n",
      "Epoch 2: Loss = 19.1405, Accuracy = 15.62%\n",
      "Epoch 3: Loss = 20.4328, Accuracy = 15.62%\n",
      "Epoch 4: Loss = 17.9636, Accuracy = 15.62%\n",
      "Epoch 5: Loss = 17.8767, Accuracy = 6.25%\n",
      "Epoch 6: Loss = 15.6003, Accuracy = 15.62%\n",
      "Epoch 7: Loss = 11.5479, Accuracy = 15.62%\n",
      "Epoch 8: Loss = 9.9866, Accuracy = 21.88%\n",
      "Epoch 9: Loss = 6.8675, Accuracy = 21.88%\n",
      "Epoch 10: Loss = 5.4709, Accuracy = 31.25%\n",
      "Epoch 11: Loss = 4.7171, Accuracy = 40.62%\n",
      "Epoch 12: Loss = 3.7913, Accuracy = 21.88%\n",
      "Epoch 13: Loss = 1.5055, Accuracy = 50.00%\n",
      "Epoch 14: Loss = 1.1423, Accuracy = 68.75%\n",
      "Epoch 15: Loss = 0.9413, Accuracy = 68.75%\n",
      "Epoch 16: Loss = 0.9015, Accuracy = 68.75%\n",
      "Epoch 17: Loss = 0.8724, Accuracy = 75.00%\n",
      "Epoch 18: Loss = 0.8468, Accuracy = 71.88%\n",
      "Epoch 19: Loss = 0.3145, Accuracy = 87.50%\n",
      "Epoch 20: Loss = 0.3242, Accuracy = 93.75%\n",
      "Epoch 21: Loss = 0.1574, Accuracy = 96.88%\n",
      "Epoch 22: Loss = 0.2539, Accuracy = 96.88%\n",
      "Epoch 23: Loss = 0.1223, Accuracy = 96.88%\n",
      "Epoch 24: Loss = 0.2796, Accuracy = 93.75%\n",
      "Epoch 25: Loss = 0.1683, Accuracy = 96.88%\n",
      "Epoch 26: Loss = 0.1295, Accuracy = 100.00%\n",
      "Epoch 27: Loss = 0.0538, Accuracy = 100.00%\n",
      "Epoch 28: Loss = 0.1183, Accuracy = 100.00%\n",
      "Epoch 29: Loss = 0.1232, Accuracy = 96.88%\n",
      "Epoch 30: Loss = 0.0320, Accuracy = 100.00%\n",
      "Epoch 31: Loss = 0.0411, Accuracy = 100.00%\n",
      "Epoch 32: Loss = 0.0196, Accuracy = 100.00%\n",
      "Epoch 33: Loss = 0.0493, Accuracy = 100.00%\n",
      "Epoch 34: Loss = 0.0418, Accuracy = 100.00%\n",
      "Epoch 35: Loss = 0.0336, Accuracy = 100.00%\n",
      "Epoch 36: Loss = 0.0640, Accuracy = 100.00%\n",
      "Epoch 37: Loss = 0.0393, Accuracy = 100.00%\n",
      "Epoch 38: Loss = 0.0153, Accuracy = 100.00%\n",
      "Epoch 39: Loss = 0.0385, Accuracy = 100.00%\n",
      "Epoch 40: Loss = 0.0206, Accuracy = 100.00%\n",
      "Epoch 41: Loss = 0.0189, Accuracy = 100.00%\n",
      "Epoch 42: Loss = 0.0346, Accuracy = 96.88%\n",
      "Epoch 43: Loss = 0.0165, Accuracy = 100.00%\n",
      "Epoch 44: Loss = 0.0092, Accuracy = 100.00%\n",
      "Model successfully overfit the small batch.\n"
     ]
    }
   ],
   "source": [
    "# Sanity check with small batch\n",
    "\n",
    "small_batch = []\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    small_batch.append((images, labels))\n",
    "    if len(small_batch) >= 1:  # Only one batch\n",
    "        break\n",
    "\n",
    "images, labels = small_batch[0]\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "model = CNNClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train on the same batch repeatedly\n",
    "for epoch in range(100):  # Fewer if it works early\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    acc = (predicted == labels).sum().item() / labels.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, Accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    if loss.item() < 0.01:\n",
    "        print(\"Model successfully overfit the small batch.\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training loop final ===\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} \"\n",
    "              f\"Val Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "train_losses, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
